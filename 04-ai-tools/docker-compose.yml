# /04-ai-tools/docker-compose.yml
#
# AI Tools Stack: Ollama (LLM Server) & AnythingLLM (RAG UI)
# Ollama can optionally use NVIDIA GPUs if prerequisites are met and config uncommented.
# AnythingLLM connects to Ollama internally.
# AnythingLLM UI is exposed via the shared 'proxy_network'.

# version: '3.8'

services:
  ollama:
    # Pinning version recommended. Check Ollama releases.
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    env_file:
      - ../.env # Load variables from root .env file
    # --- Optional: NVIDIA GPU Configuration ---
    # Uncomment the following 'deploy' section if you have NVIDIA drivers
    # and the NVIDIA Container Toolkit installed on the host.
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all # Or specify number of GPUs, e.g., 1
              capabilities: [gpu]
    # --- End Optional: NVIDIA GPU Configuration ---
    environment:
      # Values sourced from ../.env via env_file:
      - OLLAMA_KEEP_ALIVE=${OLLAMA_KEEP_ALIVE:-24h}
      - TZ=${TZ:-Etc/UTC}
      # Optional: Might help ensure container sees GPUs correctly with 'deploy' syntax
      # - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      # Persist Ollama models and data
      - ollama_data:/root/.ollama
    networks:
      # Internal network for communication with AnythingLLM
      - ai_default
    ports:
      # Expose Ollama API port only internally unless specifically needed externally
      # Caddy can proxy to this if needed, but usually access is via AnythingLLM
      - "11434" # Internal port mapping only
      # If direct host access needed: - "127.0.0.1:11434:11434" or "HOST_IP:11434:11434"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s # Give Ollama more time to start

  anythingllm:
    # Pinning version recommended. Check AnythingLLM releases.
    image: mintplexlabs/anythingllm:latest # Example: Use a specific tag if known
    container_name: anythingllm
    restart: unless-stopped
    env_file:
      - ../.env # Load TZ etc from root .env file
    volumes:
      # Persist AnythingLLM database, vector stores, configs, uploads
      - anythingllm_storage:/app/server/storage
      # Optional hot directory for document ingestion workflows
      - anythingllm_hotdir:/app/collector/hotdir
    environment:
      # Connect to Ollama using its service name on the internal network
      - OLLAMA_BASE_URL=http://ollama:11434
      # Define internal storage path (matches volume mount)
      - STORAGE_DIR=/app/server/storage
      # Value sourced from ../.env via env_file:
      - TZ=${TZ:-Etc/UTC}
    depends_on:
      ollama:
        # Wait for Ollama to be healthy before starting AnythingLLM
        condition: service_healthy
    networks:
      # Internal network to connect to Ollama
      - ai_default
      # Shared external network for Caddy reverse proxy access
      - proxy_network
    ports:
      # Expose internal port for Caddy access
      - "3001"

# Define the named volumes used by services in this file
volumes:
  ollama_data:
    driver: local
  anythingllm_storage:
    driver: local
  anythingllm_hotdir:
    driver: local

# Define the networks used by services in this file
networks:
  ai_default: # Internal network specific to this AI stack
    driver: bridge
  proxy_network: # Shared network for Caddy access
    external: true
    name: proxy_network
